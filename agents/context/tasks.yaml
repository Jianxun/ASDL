schema_version: 2

current_sprint:
  - id: T-149
    title: Diagnostics e2e conventions and manifest
    owner: Executor
    dod: >
      Create `tests/e2e/diagnostics/README.md` documenting the directory
      layout, naming (`<CODE>_<slug>`), required files per case (`case.asdl`,
      `README.md`, `expected.txt`), and command conventions (default to
      `asdlc netlist`, use `ir-dump`/`schema` when required). Document the
      pytest harness expectations (manifest-driven, `expected.txt` compares
      stderr) and required manifest fields (case directory, CLI args, expected
      exit, optional env, and `status` enum like `pending`/`ready`/`needs-harness`
      used by pytest selection). Add `tests/e2e/diagnostics/manifest.yaml`
      enumerating every diagnostic code from
      `docs/specs/spec_diagnostic_codes.md`.
    verify:
      - rg -n "PARSE-001" tests/e2e/diagnostics/manifest.yaml
    files:
      - tests/e2e/diagnostics/README.md
      - tests/e2e/diagnostics/manifest.yaml
    links:
      scratchpad: agents/scratchpads/T-149_diagnostics_e2e_conventions.md
  - id: T-150
    title: PARSE and AST diagnostics e2e cases
    owner: Executor
    depends_on: [T-149]
    dod: >
      Add case directories for PARSE-001..004 and AST-010..022 under
      `tests/e2e/diagnostics/` following the conventions. Each case includes a
      standalone `case.asdl`, a short `README.md` with the exact `asdlc`
      command, and `expected.txt` capturing stderr diagnostics. Update
      `manifest.yaml` entries with the command, exit code, status (`ready` or
      `needs-harness`), and any notes about spans or multi-file import
      fixtures.
    verify:
      - ./venv/bin/asdlc netlist tests/e2e/diagnostics/PARSE-001_yaml_parse_error/case.asdl
    files:
      - tests/e2e/diagnostics/manifest.yaml
      - tests/e2e/diagnostics/
    links:
      scratchpad: agents/scratchpads/T-150_diagnostics_parse_ast_e2e.md
  - id: T-151
    title: IR and LINT input-driven diagnostics e2e cases
    owner: Executor
    depends_on: [T-149]
    dod: >
      Add case directories for IR-001..013 and LINT-002 under
      `tests/e2e/diagnostics/` following the conventions. Each case includes
      `case.asdl`, `README.md` with the `asdlc` command, and `expected.txt`
      stderr output. Update `manifest.yaml` entries with command/exit details.
      If any code cannot be triggered via CLI without fault injection, mark the
      manifest entry `needs-harness` and explain the gap in the case README.
    verify:
      - ./venv/bin/asdlc netlist tests/e2e/diagnostics/IR-001_invalid_instance_expr/case.asdl
    files:
      - tests/e2e/diagnostics/manifest.yaml
      - tests/e2e/diagnostics/
    links:
      scratchpad: agents/scratchpads/T-151_diagnostics_ir_lint_e2e.md
  - id: T-152
    title: PASS diagnostics e2e cases
    owner: Executor
    depends_on: [T-149]
    dod: >
      Add case directories for PASS-001..004, PASS-101..106, and PASS-999 under
      `tests/e2e/diagnostics/` following the conventions. For pattern-related
      codes, supply `case.asdl`, `README.md` with the `asdlc` command, and
      `expected.txt` stderr output. For verification/crash codes that are not
      triggerable by input alone, document the limitation in the case README
      and mark the manifest entry `needs-harness`.
    verify:
      - ./venv/bin/asdlc netlist tests/e2e/diagnostics/PASS-101_invalid_numeric_range/case.asdl
    files:
      - tests/e2e/diagnostics/manifest.yaml
      - tests/e2e/diagnostics/
    links:
      scratchpad: agents/scratchpads/T-152_diagnostics_pass_e2e.md
  - id: T-153
    title: EMIT diagnostics e2e cases
    owner: Executor
    depends_on: [T-149]
    dod: >
      Add case directories for EMIT-001..013 under `tests/e2e/diagnostics/`
      following the conventions. Use `asdlc netlist` (and per-case backend
      config overrides when required) to trigger each code. Each case includes
      `case.asdl`, `README.md` with the exact command (including any
      `ASDL_BACKEND_CONFIG`), and `expected.txt` stderr output. Update
      `manifest.yaml` entries with command/exit details and status.
    verify:
      - ./venv/bin/asdlc netlist tests/e2e/diagnostics/EMIT-001_top_missing/case.asdl
    files:
      - tests/e2e/diagnostics/manifest.yaml
      - tests/e2e/diagnostics/
    links:
      scratchpad: agents/scratchpads/T-153_diagnostics_emit_e2e.md
  - id: T-154
    title: TOOL diagnostics e2e cases
    owner: Executor
    depends_on: [T-149]
    dod: >
      Add case directories for TOOL-001..003 under `tests/e2e/diagnostics/`
      following the conventions. Use direct `asdlc` commands that can be run
      manually (for example, schema output to a read-only path for TOOL-002).
      Where a code cannot be triggered without environment fault injection,
      document the manual steps in the case README and mark the manifest entry
      `needs-harness`.
    verify:
      - ./venv/bin/asdlc schema --out /root
    files:
      - tests/e2e/diagnostics/manifest.yaml
      - tests/e2e/diagnostics/
    links:
      scratchpad: agents/scratchpads/T-154_diagnostics_tool_e2e.md
  - id: T-155
    title: Pytest harness for diagnostics e2e cases
    owner: Executor
    depends_on: [T-149]
    dod: >
      Add `tests/e2e/diagnostics/test_diagnostics_e2e.py` that reads
      `tests/e2e/diagnostics/manifest.yaml` and runs `asdlc` commands for each
      `ready` case (skip `pending`/`needs-harness`). The test must assert exit
      codes and compare stderr to `expected.txt` (normalize trailing newlines
      only). Support per-case env overrides defined in the manifest. Update the
      diagnostics README with the pytest invocation and how to update
      `expected.txt`.
    verify:
      - venv/bin/pytest tests/e2e/diagnostics/test_diagnostics_e2e.py -v
    files:
      - tests/e2e/diagnostics/test_diagnostics_e2e.py
      - tests/e2e/diagnostics/README.md
      - tests/e2e/diagnostics/manifest.yaml
    links:
      scratchpad: agents/scratchpads/T-155_diagnostics_e2e_pytest.md

backlog: []

exploration_candidates:
  - IFIR diagnostics spans: mapping strategy, edge cases, and test fixtures.
  - CLI pipeline UX: command flow, error messaging, and file layout.
  - Netlist emission validation: strictness vs warnings; validation stages.
  - xDSL pipeline boundaries: pass split/merge opportunities and IR boundaries.
  - Codebase map gaps: navigation audit for future Executors.
